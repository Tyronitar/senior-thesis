@article{
    asadi2022worstcase,
    title={Worst-Case to Average-Case Reductions via Additive Combinatorics}, 
    author={Vahid R. Asadi and Alexander Golovnev and Tom Gur and Igor Shinkar},
    year={2022},
    eprint={2202.08996},
    archivePrefix={arXiv},
    primaryClass={cs.DS},
    doi={10.48550/arXiv.2202.08996}
}


@article{
    Chang2002,
    author = {Mei-Chu Chang},
    title = {{A polynomial bound in Freiman's theorem}},
    volume = {113},
    journal = {Duke Mathematical Journal},
    number = {3},
    publisher = {Duke University Press},
    pages = {399 -- 419},
    year = {2002},
    doi = {10.1215/S0012-7094-02-11331-3},
    URL = {https://doi.org/10.1215/S0012-7094-02-11331-3}
}


@article{Levin1986,
author = {Levin, Leonid A.},
title = {Average Case Complete Problems},
journal = {SIAM Journal on Computing},
volume = {15},
number = {1},
pages = {285-286},
year = {1986},
doi = {10.1137/0215020},

URL = { 
        https://doi.org/10.1137/0215020
},
eprint = { 
        https://doi.org/10.1137/0215020
}
,
    abstract = { Many interesting combinatorial problems were found to be NP-complete. Since there is little hope to solve them fast in the worst case, researchers look for algorithms which are fast just “on average”. This matter is sensitive to the choice of a particular NP-complete problem and a probability distribution of its instances. Some of these tasks were easy and some not. But one needs a way to distinguish the “difficult on average” problems. Such negative results could not only save “positive” efforts but may also be used in areas (like cryptography) where hardness of some problems is a frequent assumption. It is shown below that the Tiling problem with uniform distribution of instances has no polynomial “on average” algorithm, unless every NP-problem with every simple probability distribution has it. It is interesting to try to prove similar statements for other NP-problems which resisted so far “average case” attacks. }
}


@misc{bogdanov2021averagecase,
      title={Average-Case Complexity}, 
      author={Andrej Bogdanov and Luca Trevisan},
      year={2021},
      eprint={cs/0606037},
      archivePrefix={arXiv},
      primaryClass={cs.CC}
}

@article{Sanders2012,
	doi = {10.2140/apde.2012.5.627},
	url = {https://doi.org/10.2140\%2Fapde.2012.5.627},
	year = 2012,
	month = {oct},
	publisher = {Mathematical Sciences Publishers},
	volume = {5},
	number = {3},
	pages = {627--655},
	author = {Tom Sanders},
	title = {On the Bogolyubov{\textendash}Ruzsa lemma},
	journal = {Analysis {\&}amp$\mathsemicolon$ {PDE}
}
}

@article{BLUM1993549,
title = {Self-testing/correcting with applications to numerical problems},
journal = {Journal of Computer and System Sciences},
volume = {47},
number = {3},
pages = {549-595},
year = {1993},
issn = {0022-0000},
doi = {https://doi.org/10.1016/0022-0000(93)90044-W},
url = {https://www.sciencedirect.com/science/article/pii/002200009390044W},
author = {Manuel Blum and Michael Luby and Ronitt Rubinfeld},
abstract = {Suppose someone gives us an extremely fast program P that we can call as a black box to compute a function f. Should we trust that P works correctly? A self-testing/correcting pair for f allows us to: (1) estimate the probability that P(x) ≠ φ(x) when x is randomly chosen; (2) on any input x, compute f(x) correctly as long as P is not too faulty on average. Furthermore, both (1) and (2) take time only slightly more than the original running time of P. We present general techniques for constructing simple to program self-testing/correcting pairs for a variety of numerical functions, including integer multiplication, modular multiplication, matrix multiplication, inverting matrices, computing the determinant of a matrix, computing the rank of a matrix, integer division, modular exponentiation, and polynomial multiplication.}
}


@book{Lovett2017,
 author = {Lovett, Shachar},
 title = {Additive Combinatorics and its Applications in Theoretical Computer Science},
 year = {2017},
 pages = {1--55},
 doi = {10.4086/toc.gs.2017.008},
 publisher = {Theory of Computing Library},
 number = {8},
 series = {Graduate Surveys},
 URL = {http://www.theoryofcomputing.org/library.html},
}

@article{Trevisan2009,
author = {Trevisan, Luca},
title = {Guest Column: Additive Combinatorics and Theoretical Computer Science},
year = {2009},
issue_date = {June 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {2},
issn = {0163-5700},
url = {https://doi.org/10.1145/1556154.1556170},
doi = {10.1145/1556154.1556170},
abstract = {Additive combinatorics is the branch of combinatorics where the objects of study are subsets of the integers or of other abelian groups, and one is interested in properties and patterns that can be expressed in terms of linear equations. More generally, arithmetic combinatorics deals with properties and patterns that can be expressed via additions and multiplications.In the past ten years, additive and arithmetic combinatorics have been extremely successful areas of mathematics, featuring a convergence of techniques from graph theory, analysis and ergodic theory. They have helped prove long-standing open questions in additive number theory, and they offer much promise of future progress.Techniques from additive and arithmetic combinatorics have found several applications in computer science too, to property testing, pseudorandomness, PCP constructions, lower bounds, and extractor constructions. Typically, whenever a technique from additive or arithmetic combinatorics becomes understood by computer scientists, it finds some application.Considering that there is still a lot of additive and arithmetic combinatorics that computer scientists do not understand (and, the field being very active, even more will be developed in the near future), there seems to be much potential for future connections and applications.},
journal = {SIGACT News},
month = {jun},
pages = {50–66},
numpages = {17}
}